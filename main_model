import re
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertModel
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
from torch_geometric.nn import GCNConv


# 1. Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)


# 2. Helper functions


def detect_language(token: str) -> str:
   """
   Detect language for a token by checking Bengali script range.
   Returns 'bn' for Bengali, 'en' otherwise.
   """
   return 'bn' if re.search(r'[\u0980-\u09FF]', token) else 'en'


def get_cs_count(text: str) -> int:
   """
   Count the number of times we switch from one language to another
   in the tokenized text.
   """
   tokens = text.split()
   if len(tokens) < 2:
       return 0
   langs = [detect_language(tok) for tok in tokens]
   return sum(1 for i in range(1, len(langs)) if langs[i] != langs[i - 1])


def preprocess_text(text: str) -> str:
   """
   Basic text preprocessing: remove extra spaces, repeated punctuation, etc.
   """
   text = text.strip()
   text = re.sub(r'\s+', ' ', text)
   text = re.sub(r'([!?.]){2,}', r'\1', text)
   return text


def build_block_diagonal_chain_graph(batch_size, seq_len):
   """
   Build a block-diagonal adjacency matrix that connects tokens in a chain for each sample.
   - We have total (batch_size * seq_len) nodes.
   - For each sample in the batch, we connect token i -> i+1 and i+1 -> i for i in [0..seq_len-2].
   """
   # Number of total nodes
   N = batch_size * seq_len
  
   row = []
   col = []
   # For each sample in the batch
   for b in range(batch_size):
       offset = b * seq_len
       # Connect consecutive tokens
       for i in range(seq_len - 1):
           # i -> i+1
           row.append(offset + i)
           col.append(offset + i + 1)
           # i+1 -> i
           row.append(offset + i + 1)
           col.append(offset + i)
   edge_index = torch.tensor([row, col], dtype=torch.long)
   return edge_index


# 3. Load dataset
csv_file = "/dataset.csv"  
df = pd.read_csv(csv_file)


# Make sure there's a 'comment_text' column and a 'label' column in your CSV.
df["comment_text"] = df["comment_text"].fillna("").apply(preprocess_text)


# 4. Encode labels
label_encoder = LabelEncoder()
df["label"] = label_encoder.fit_transform(df["label"])


# Convert DataFrame to list of dicts
data = df.to_dict(orient="records")


# 5. Create Train / Val / Test splits
# First do train vs test
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
# Then split train_data further into train vs val
train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42)
# So effectively: 60% train, 20% val, 20% test


# 6. Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")


# 7. Define PyTorch Dataset
class CodeMixedSentimentDataset(Dataset):
   def __init__(self, data, tokenizer, max_length=64):
       self.data = data
       self.tokenizer = tokenizer
       self.max_length = max_length


   def __len__(self):
       return len(self.data)


   def __getitem__(self, idx):
       sample = self.data[idx]
       text = sample["comment_text"]
       cs_count = get_cs_count(text)
       encoding = self.tokenizer(
           text,
           add_special_tokens=True,
           truncation=True,
           max_length=self.max_length,
           padding="max_length",
           return_tensors="pt"
       )
       # Squeeze batch dimension from tokenizer outputs
       item = {key: encoding[key].squeeze(0) for key in encoding}
       item["label"] = torch.tensor(sample["label"], dtype=torch.long)
       item["cs_count"] = torch.tensor(cs_count, dtype=torch.float)
       return item


# 8. Create Dataset and DataLoader objects
max_length = 64
batch_size = 16


train_dataset = CodeMixedSentimentDataset(train_data, tokenizer, max_length=max_length)
val_dataset   = CodeMixedSentimentDataset(val_data,   tokenizer, max_length=max_length)
test_dataset  = CodeMixedSentimentDataset(test_data,  tokenizer, max_length=max_length)


train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)
test_dataloader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)


# ---- Dual Attention Mechanism
class DualAttention(nn.Module):
   def __init__(self, input_dim, attn_dim):
       super(DualAttention, self).__init__()
       self.linear_t = nn.Linear(input_dim, attn_dim)
       self.linear_g = nn.Linear(input_dim, attn_dim)
       self.context_vector = nn.Parameter(torch.randn(attn_dim))
       self.combine = nn.Linear(input_dim * 2, input_dim)


   def forward(self, bert_out, gcn_out, mask):
       """
       bert_out: (B, S, H)
       gcn_out: (B, S, H)
       mask: (B, S) -> 1 for valid tokens, 0 for padding
       """
       B, S, H = bert_out.shape
      
       # (B, S, attn_dim)
       proj_t = torch.tanh(self.linear_t(bert_out))
       proj_g = torch.tanh(self.linear_g(gcn_out))
      
       # (B, S)
       score_t = torch.einsum("bsa,a->bs", proj_t, self.context_vector)
       score_g = torch.einsum("bsa,a->bs", proj_g, self.context_vector)
      
       # mask out padding positions so they don't contribute
       score_t = score_t.masked_fill(mask == 0, float('-inf'))
       score_g = score_g.masked_fill(mask == 0, float('-inf'))
      
       # (B, S, 1)
       attn_weights_t = torch.softmax(score_t, dim=1).unsqueeze(-1)
       attn_weights_g = torch.softmax(score_g, dim=1).unsqueeze(-1)
      
       # Weighted sum -> (B, H)
       context_t = torch.sum(bert_out * attn_weights_t, dim=1)
       context_g = torch.sum(gcn_out * attn_weights_g, dim=1)
      
       # Combine -> (B, H)
       combined = torch.cat([context_t, context_g], dim=-1)  # (B, 2H)
       combined = torch.tanh(self.combine(combined))         # (B, H)
       return combined


# ---- Graph Enhanced Model
class GraphEnhancedSentimentModel(nn.Module):
   def __init__(self, hidden_dim=768, attn_dim=256, num_classes=3):
       super(GraphEnhancedSentimentModel, self).__init__()
       self.bert = BertModel.from_pretrained("bert-base-multilingual-cased")
       self.gcn1 = GCNConv(hidden_dim, hidden_dim)
       self.gcn2 = GCNConv(hidden_dim, hidden_dim)
       self.dual_attention = DualAttention(hidden_dim, attn_dim)
       self.classifier = nn.Linear(hidden_dim, num_classes)
       self.aux_head = nn.Linear(hidden_dim, 1)


   def forward(self, input_ids, attention_mask, edge_index):
       """
       input_ids: (B, S)
       attention_mask: (B, S)
       edge_index: (2, E) for block-diagonal chain graph across B*S nodes
       """
       # 1) BERT
       outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
       bert_out = outputs.last_hidden_state  # (B, S, hidden_dim)
      
       B, S, H = bert_out.shape
      
       # 2) Flatten for GCN
       flatten_bert = bert_out.view(B * S, H)
      
       # 3) GCN
       gcn_out = self.gcn1(flatten_bert, edge_index)
       gcn_out = self.gcn2(gcn_out, edge_index)
      
       # Reshape back: (B, S, H)
       gcn_out = gcn_out.view(B, S, H)
      
       # 4) Dual attention
       fused_output = self.dual_attention(bert_out, gcn_out, attention_mask)
      
       # 5) Outputs
       sentiment_logits = self.classifier(fused_output)  # (B, num_classes)
       cs_pred = self.aux_head(fused_output).squeeze(-1) # (B,)
      
       return sentiment_logits, cs_pred


# Instantiate model
model = GraphEnhancedSentimentModel(num_classes=len(label_encoder.classes_)).to(device)


# Define Losses
criterion_sentiment = nn.CrossEntropyLoss()
criterion_cs = nn.MSELoss()


# Define Optimizer
optimizer = optim.Adam(model.parameters(), lr=2e-5)


# Training loop with validation
num_epochs = 5
best_val_f1 = 0.0
best_model_path = "best_model.pt"


for epoch in range(num_epochs):
   model.train()
   total_train_loss = 0.0
  
   for batch in train_dataloader:
       input_ids = batch["input_ids"].to(device)
       attention_mask = batch["attention_mask"].to(device)
       labels = batch["label"].to(device)
       cs_labels = batch["cs_count"].to(device)


       B, S = input_ids.shape
       edge_index = build_block_diagonal_chain_graph(B, S).to(device)
      
       optimizer.zero_grad()
      
       sentiment_logits, cs_pred = model(input_ids, attention_mask, edge_index)
      
       loss_sentiment = criterion_sentiment(sentiment_logits, labels)
       loss_cs = criterion_cs(cs_pred, cs_labels)
       loss = loss_sentiment + loss_cs
      
       loss.backward()
       optimizer.step()
      
       total_train_loss += loss.item()
  
   avg_train_loss = total_train_loss / len(train_dataloader)
  
   #######################
   # Validation phase    #
   #######################
   model.eval()
   total_val_loss = 0.0
   val_preds = []
   val_labels_list = []
  
   with torch.no_grad():
       for batch in val_dataloader:
           input_ids = batch["input_ids"].to(device)
           attention_mask = batch["attention_mask"].to(device)
           labels = batch["label"].to(device)
           cs_labels = batch["cs_count"].to(device)


           B, S = input_ids.shape
           edge_index = build_block_diagonal_chain_graph(B, S).to(device)


           sentiment_logits, cs_pred = model(input_ids, attention_mask, edge_index)
          
           loss_sentiment = criterion_sentiment(sentiment_logits, labels)
           loss_cs = criterion_cs(cs_pred, cs_labels)
           loss = loss_sentiment + loss_cs
           total_val_loss += loss.item()


           preds = torch.argmax(sentiment_logits, dim=1).cpu().numpy()
           val_preds.extend(preds)
           val_labels_list.extend(labels.cpu().numpy())


   avg_val_loss = total_val_loss / len(val_dataloader)
   val_accuracy = accuracy_score(val_labels_list, val_preds)
   val_f1 = f1_score(val_labels_list, val_preds, average='macro')
  
   print(f"Epoch [{epoch+1}/{num_epochs}]")
   print(f"  Train Loss: {avg_train_loss:.4f}")
   print(f"  Val   Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}")
  
   # Save the best model if val F1 improves
   if val_f1 > best_val_f1:
       best_val_f1 = val_f1
       torch.save(model.state_dict(), best_model_path)
       print("  --> Saved new best model!")


# Load the best model before final test
model.load_state_dict(torch.load(best_model_path))


# Final evaluation on the test set
model.eval()
all_preds = []
all_labels = []


with torch.no_grad():
   for batch in test_dataloader:
       input_ids = batch["input_ids"].to(device)
       attention_mask = batch["attention_mask"].to(device)
       labels = batch["label"].to(device)


       B, S = input_ids.shape
       edge_index = build_block_diagonal_chain_graph(B, S).to(device)


       sentiment_logits, cs_pred = model(input_ids, attention_mask, edge_index)
      
       preds = torch.argmax(sentiment_logits, dim=1).cpu().numpy()
       all_preds.extend(preds)
       all_labels.extend(labels.cpu().numpy())


accuracy = accuracy_score(all_labels, all_preds)
f1 = f1_score(all_labels, all_preds, average='macro')
precision = precision_score(all_labels, all_preds, average='macro')
recall = recall_score(all_labels, all_preds, average='macro')


print(f"\nTest Accuracy:   {accuracy:.4f}")
print(f"Test F1 (macro): {f1:.4f}")
print(f"Precision:       {precision:.4f}")
print(f"Recall:          {recall:.4f}")


# Confusion Matrix
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(5,5))
sns.heatmap(cm, annot=True, fmt='d',
           xticklabels=label_encoder.classes_,
           yticklabels=label_encoder.classes_)
plt.title("Confusion Matrix")
plt.ylabel("True Label")
plt.xlabel("Predicted Label")
plt.show()



