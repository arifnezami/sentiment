import re
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertModel
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import seaborn as sns

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def detect_language(token):
    return 'bn' if re.search(r'[\u0980-\u09FF]', token) else 'en'

def get_cs_count(text):
    tokens = text.split()
    if len(tokens) < 2:
        return 0
    langs = [detect_language(tok) for tok in tokens]
    return sum(1 for i in range(1, len(langs)) if langs[i] != langs[i-1])

def preprocess_text(text):
    text = text.strip()
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'([!?.]){2,}', r'\1', text)
    return text

csv_file = "https://docs.google.com/spreadsheets/d/e/2PACX-1vRFVaLgFhrLZ4xsRpUFj_bXtuzqznNT8RIF5ZuqUNaxU94_bUXVml89lY3Kd8P8DsT9J85Uo_UvfExf/pub?gid=314125647&single=true&output=csv"
df = pd.read_csv(csv_file)
df["comment_text"] = df["comment_text"].fillna("").apply(preprocess_text)

label_encoder = LabelEncoder()
df["label"] = label_encoder.fit_transform(df["label"])

data = df.to_dict(orient="records")

train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")

class CodeMixedSentimentDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=64):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]
        text = sample["comment_text"]
        cs_count = get_cs_count(text)
        encoding = self.tokenizer(
            text, add_special_tokens=True, truncation=True,
            max_length=self.max_length, padding="max_length", return_tensors="pt"
        )
        item = {key: encoding[key].squeeze(0) for key in encoding}
        item["label"] = torch.tensor(sample["label"], dtype=torch.long)
        item["cs_count"] = torch.tensor(cs_count, dtype=torch.float)
        item["text"] = text
        return item

batch_size = 8
train_dataset = CodeMixedSentimentDataset(train_data, tokenizer)
test_dataset = CodeMixedSentimentDataset(test_data, tokenizer)

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

class DualAttention(nn.Module):
    def __init__(self, input_dim, attn_dim):
        super(DualAttention, self).__init__()
        self.linear_t = nn.Linear(input_dim, attn_dim)
        self.linear_g = nn.Linear(input_dim, attn_dim)
        self.context_vector = nn.Parameter(torch.randn(attn_dim))
        self.combine = nn.Linear(input_dim * 2, input_dim)

    def forward(self, bert_out, gcn_out, mask):
        proj_t = torch.tanh(self.linear_t(bert_out))
        proj_g = torch.tanh(self.linear_g(gcn_out))
        score_t = torch.matmul(proj_t, self.context_vector)
        score_g = torch.matmul(proj_g, self.context_vector)
        score_t = score_t.masked_fill(mask == 0, -1e9)
        score_g = score_g.masked_fill(mask == 0, -1e9)
        attn_weights_t = torch.softmax(score_t, dim=1).unsqueeze(-1)
        attn_weights_g = torch.softmax(score_g, dim=1).unsqueeze(-1)
        context_t = torch.sum(bert_out * attn_weights_t, dim=1)
        context_g = torch.sum(gcn_out * attn_weights_g, dim=1)
        combined = torch.cat([context_t, context_g], dim=-1)
        return torch.tanh(self.combine(combined))

class GraphEnhancedSentimentModel(nn.Module):
    def __init__(self, hidden_dim=768, attn_dim=256, num_classes=3):
        super(GraphEnhancedSentimentModel, self).__init__()
        self.bert = BertModel.from_pretrained("bert-base-multilingual-cased")
        self.gcn = nn.Linear(hidden_dim, hidden_dim)
        self.dual_attention = DualAttention(hidden_dim, attn_dim)
        self.classifier = nn.Linear(hidden_dim, num_classes)
        self.aux_head = nn.Linear(hidden_dim, 1)

    def forward(self, input_ids, attention_mask):
        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        bert_out = bert_outputs.last_hidden_state
        gcn_out = self.gcn(bert_out)
        fused_output = self.dual_attention(bert_out, gcn_out, attention_mask)
        return self.classifier(fused_output), self.aux_head(fused_output).squeeze(1)

model = GraphEnhancedSentimentModel().to(device)
criterion_sentiment = nn.CrossEntropyLoss()
criterion_cs = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-5)
